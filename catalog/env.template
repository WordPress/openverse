#     ,-.  ;-.  ,--. .  . .   , ,--. ,-.   ,-.  ,--.    ,-.  ,-.  .  . ,--. ,  ,-.     #
#    /   \ |  ) |    |\ | |  /  |    |  ) (   ` |      /    /   \ |\ | |    | /        #
#    |   | |-'  |-   | \| | /   |-   |-<   `-.  |-     |    |   | | \| |-   | | -.     #
#    \   / |    |    |  | |/    |    |  \ .   ) |      \    \   / |  | |    | \  |     #
#     `-'  '    `--' '  ' '     `--' '  '  `-'  `--'    `-'  `-'  '  ' '    '  `-'     #

########################################################################################
# Airflow Settings
########################################################################################
# Some brand-based suggestions: #C52B9B (pink), #FFE033 (yellow)
AIRFLOW__WEBSERVER__NAVBAR_COLOR="#FFF"
# Disabled by default to make development easier
# (enabled on prod for security)
AIRFLOW__CORE__HIDE_SENSITIVE_VAR_CONN_FIELDS=False
# Use the following python code to generate a fernet key for production
# python -c "import base64, os; print(base64.urlsafe_b64encode(os.urandom(32)).decode())"
# AIRFLOW__CORE__FERNET_KEY=
# CSRF key https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.html#secret-key
AIRFLOW__WEBSERVER__SECRET_KEY=sample-secret-key=
# Executor to use
AIRFLOW__CORE__EXECUTOR=LocalExecutor
# Environment this instance is being run in
AIRFLOW_VAR_ENVIRONMENT=dev

########################################################################################
# API Keys
########################################################################################
# See: https://airflow.apache.org/docs/apache-airflow/stable/howto/variable.html#storing-variables-in-environment-variables
AIRFLOW_VAR_API_KEY_BROOKLYN_MUSEUM=not_set
AIRFLOW_VAR_API_KEY_DATA_GOV=not_set
AIRFLOW_VAR_API_KEY_EUROPEANA=not_set
AIRFLOW_VAR_API_KEY_FLICKR=not_set
AIRFLOW_VAR_API_KEY_FREESOUND=not_set
AIRFLOW_VAR_API_KEY_JAMENDO=not_set
AIRFLOW_VAR_API_KEY_NYPL=not_set
AIRFLOW_VAR_API_KEY_RAWPIXEL=not_set
AIRFLOW_VAR_API_KEY_THINGIVERSE=not_set
AIRFLOW_VAR_API_KEY_WALTERS_ART_MUSEUM=not_set


########################################################################################
# Connection/Variable info
########################################################################################
# Airflow primary metadata database
# Change the following line in prod to use the appropriate DB
AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@upstream_db:5432/airflow
# Remote logging connection ID
# Replace "access_key" and "secret+key" with the real values. Secret key must be URL-encoded
AIRFLOW_CONN_AWS_DEFAULT=aws://test_key:test_secret@?region_name=us-east-1&endpoint_url=http://s3:5000

# Catalog DB connection. Change the following line in prod to use the appropriate DB
AIRFLOW_CONN_POSTGRES_OPENLEDGER_UPSTREAM=postgres://deploy:deploy@upstream_db:5432/openledger
AIRFLOW_CONN_POSTGRES_OPENLEDGER_TESTING=postgres://deploy:deploy@upstream_db:5432/openledger
OPENLEDGER_CONN_ID=postgres_openledger_upstream
TEST_CONN_ID=postgres_openledger_testing

# API DB connection. Change the following line in prod to use the appropriate DB
AIRFLOW_CONN_POSTGRES_OPENLEDGER_API=postgres://deploy:deploy@db:5432/openledger
OPENLEDGER_API_CONN_ID=postgres_openledger_api

# Slack webhook connection info (note that these values are modified by the Docker entrypoint)
# A distinction is made here between "notifications" and "alerts", the former being
# useful updates and the latter being alarms or actionable errors.
AIRFLOW_CONN_SLACK_NOTIFICATIONS=https://slack
AIRFLOW_CONN_SLACK_ALERTS=https://slack

S3_LOCAL_ENDPOINT=http://s3:5000
AWS_CONN_ID=aws_default

AIRFLOW_CONN_EMR_EMPTY=emr://
AIRFLOW_CONN_EMR_TEST=emr://?host=http://s3:5000

EMR_CONN_ID=emr_empty
EMR_TEST_CONN_ID=emr_test

# Connection to the Ingestion Server, used for managing data refreshes. Default is used to
# connect to your locally running ingestion server.
AIRFLOW_CONN_DATA_REFRESH=http://ingestion_server:8001

# Django Admin url. Change the following line to use the appropriate environment.
DJANGO_ADMIN_URL="https://localhost:8000/admin"

# GitHub - used for maintenance
AIRFLOW_VAR_GITHUB_API_KEY="not_set"


########################################################################################
# Other config
########################################################################################
# Version of the catalog docker image to use. Defaults to `latest` and is not used for
# local development (since the image is always built locally). See available tags at
# https://ghcr.io/wordpress/openverse-catalog
DOCKER_IMAGE_TAG=latest
# External port airflow will be mounted to
AIRFLOW_PORT=9090
# Minutes to wait until processing a file that hasn't been modified
LOADER_FILE_AGE=1
# Contact email for any APIs
CONTACT_EMAIL=openverse@wordpress.org
# AWS/S3 configuration - does not need to be changed for development
AWS_ACCESS_KEY=test_key
AWS_SECRET_KEY=test_secret
# General bucket used for TSV->DB ingestion and logging
OPENVERSE_BUCKET=openverse-storage
# Seconds to wait before poking for availability of the data refresh pool when running a data_refresh
# DAG. Used to shorten the time for testing purposes.
DATA_REFRESH_POKE_INTERVAL=5
# Number of Retries if DAG task fails to run
DEFAULT_RETRY_COUNT = 2

AIRFLOW_VAR_AIRFLOW_RDS_ARN=unset
AIRFLOW_VAR_AIRFLOW_RDS_SNAPSHOTS_TO_RETAIN=7
