"""
# Data Refresh DAG Factory
This file generates our data refresh DAGs using a factory function.
These DAGs initiate a data refresh for a given media type and awaits the
success or failure of the refresh. Importantly, they are also configured to
ensure that no two data refresh DAGs can run concurrently, as required by
the server.

A data refresh occurs on the data refresh server in the openverse-api project.
This is a task which imports data from the upstream Catalog database into the
API, copies contents to a new Elasticsearch index, and makes the index "live".
This process is necessary to make new content added to the Catalog by our
provider DAGs available on the frontend. You can read more in the [README](
https://github.com/WordPress/openverse-api/blob/main/ingestion_server/README.md
)

The DAGs generated by this factory allow us to schedule those refreshes through
Airflow. Since no two refreshes can run simultaneously, all tasks are run in a
special `data_refresh` pool with a single worker slot. To ensure that tasks
run in an acceptable order (ie the trigger step for one DAG cannot run if a
previously triggered refresh is still running), each DAG has the following
steps:

1. The `wait_for_data_refresh` step uses a custom Sensor that will wait until
none of the `external_dag_ids` (corresponding to the other data refresh DAGs)
are 'running'. A DAG is considered to be 'running' if it is itself in the
RUNNING state __and__ its own `wait_for_data_refresh` step has completed
successfully. The Sensor suspends itself and frees up the worker slot if
another data refresh DAG is running.

2. The `trigger_data_refresh` step then triggers the data refresh by POSTing
to the `/task` endpoint on the data refresh server with relevant data. A
successful response will include the `status_check` url used to check on the
status of the refresh, which is passed on to the next task via XCom.

3. Finally the `wait_for_data_refresh` task waits for the data refresh to be
complete by polling the `status_url`. Note this task does not need to be
able to suspend itself and free the worker slot, because we want to lock the
entire pool on waiting for a particular data refresh to run.

You can find more background information on this process in the following
issues and related PRs:

- [[Feature] Data refresh orchestration DAG](
https://github.com/WordPress/openverse-catalog/issues/353)
"""
import json
import logging
import os
from typing import Sequence
from urllib.parse import urlparse

from airflow import DAG
from airflow.exceptions import AirflowException
from airflow.providers.http.operators.http import SimpleHttpOperator
from airflow.providers.http.sensors.http import HttpSensor
from common.constants import DAG_DEFAULT_ARGS, XCOM_PULL_TEMPLATE
from common.sensors.single_run_external_dags_sensor import SingleRunExternalDAGsSensor
from data_refresh.data_refresh_types import DATA_REFRESH_CONFIGS, DataRefresh
from requests import Response


logger = logging.getLogger(__name__)


DATA_REFRESH_POOL = "data_refresh"


def response_filter_data_refresh(response: Response) -> str:
    """
    Filter for the `trigger_data_refresh` task, used to grab the endpoint needed
    to poll for the status of the triggered data refresh. This information will
    then be available via XCom in the downstream tasks.
    """
    status_check_url = response.json()["status_check"]
    return urlparse(status_check_url).path


# Response check to the `wait_for_completion` Sensor. Processes the response to
# determine whether the task can complete.
def response_check_wait_for_completion(response: Response) -> bool:
    """
    Response check to the `wait_for_completion` Sensor. Processes the response to
    determine whether the task can complete.
    """
    data = response.json()

    if data["active"]:
        # The data refresh is still running. Poll again later.
        return False

    if data["error"]:
        raise AirflowException("Error triggering data refresh.")

    logger.info(
        f"Data refresh done with {data['percent_completed']}% \
        completed."
    )
    return True


def create_data_refresh_dag(data_refresh: DataRefresh, external_dag_ids: Sequence[str]):
    """
    This factory method instantiates a DAG that will run the data refresh for
    the given `media_type`.

    A data refresh runs for a given media type in the API DB. It imports the
    data for that type from the upstream DB in the Catalog, reindexes the data,
    and updates and reindex Elasticsearch.

    A data refresh can only be performed for one media type at a time, so the DAG
    must also use a Sensor to make sure that no two data refresh tasks run
    concurrently.

    It is intended that the data_refresh tasks, or at least the initial
    `wait_for_data_refresh` tasks, should be run in a custom pool with 1 worker
    slot. This enforces that no two `wait_for_data_refresh` tasks can start
    concurrently and enter a race condition.

    Required Arguments:

    data_refresh:     dataclass containing configuration information for the
                      DAG
    external_dag_ids: list of ids of the other data refresh DAGs. This DAG
                      will not run concurrently with any dependent DAG.
    """
    default_args = {
        **DAG_DEFAULT_ARGS,
        **data_refresh.default_args,
        "pool": DATA_REFRESH_POOL,
    }
    poke_interval = int(os.getenv("DATA_REFRESH_POKE_INTERVAL", 60 * 15))
    dag = DAG(
        dag_id=data_refresh.dag_id,
        default_args=default_args,
        start_date=data_refresh.start_date,
        schedule_interval=data_refresh.schedule_interval,
        max_active_runs=1,
        catchup=False,
        doc_md=__doc__,
        tags=["data_refresh"],
    )

    with dag:
        # Wait to ensure that no other Data Refresh DAGs are running.
        wait_for_data_refresh = SingleRunExternalDAGsSensor(
            task_id="wait_for_data_refresh",
            external_dag_ids=external_dag_ids,
            check_existence=True,
            dag=dag,
            poke_interval=poke_interval,
            mode="reschedule",
        )

        data_refresh_post_data = {
            "model": data_refresh.media_type,
            "action": "INGEST_UPSTREAM",
        }

        # Trigger the refresh on the data refresh server.
        trigger_data_refresh = SimpleHttpOperator(
            task_id="trigger_data_refresh",
            http_conn_id="data_refresh",
            endpoint="task",
            method="POST",
            headers={"Content-Type": "application/json"},
            data=json.dumps(data_refresh_post_data),
            response_check=lambda response: response.status_code == 202,
            response_filter=response_filter_data_refresh,
            dag=dag,
        )

        # Wait for the data refresh to complete.
        wait_for_completion = HttpSensor(
            task_id="wait_for_completion",
            http_conn_id="data_refresh",
            endpoint=XCOM_PULL_TEMPLATE.format(
                trigger_data_refresh.task_id, "return_value"
            ),
            method="GET",
            response_check=response_check_wait_for_completion,
            dag=dag,
            mode="reschedule",
            poke_interval=poke_interval,
            timeout=(60 * 60 * 24 * 3),  # 3 days
        )

        wait_for_data_refresh >> trigger_data_refresh >> wait_for_completion

    return dag


all_data_refresh_dag_ids = {refresh.dag_id for refresh in DATA_REFRESH_CONFIGS}

for data_refresh in DATA_REFRESH_CONFIGS:
    # Construct a set of all data refresh DAG ids other than the current DAG
    other_dag_ids = all_data_refresh_dag_ids - {data_refresh.dag_id}

    # Generate the DAG for this config, dependent on all the others
    globals()[data_refresh.dag_id] = create_data_refresh_dag(
        data_refresh, other_dag_ids
    )
